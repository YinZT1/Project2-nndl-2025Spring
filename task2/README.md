### 本项目并不需要README
task2的多个py文件代表了我运行的全过程。每个文件都可以单独运行。

如果你需要运行，请修改其中文件的路径即可。

本项目探索了Batch-Norm对网络训练的影响，得到以下结论：

1.  **加速训练收敛：** 从训练损失和测试准确率曲线可以看出，加入 BN 层的模型收敛速度远快于没有 BN 层的模型。这是因为 BN 层通过规范化层输入，使得损失函数的梯度更稳定和可预测。
2.  **提升模型性能：** 加入 BN 层的模型在测试集上达到了更高的准确率，说明 BN 层有助于模型学习到更好的特征表示，并可能具有一定的正则化效果，提高了模型的泛化能力。
3. 改善损失函数的地形：
	- BN 层使得损失函数的优化路径更加平滑和直接。虽然 `BN_SGD` 的可视化地形在全局看起来陡峭，但它可能形成了一个引导优化器高效到达最优解的清晰路径。
	- 相比之下，没有 BN 层的模型其损失地形可能包含更多使优化变慢的区域 (如平坦区域或不规则的梯度变化)，导致收敛缓慢且性能较差。
4. **稳定训练过程：** BN 层降低了模型对初始化参数的敏感性，并允许使用更高的学习率，从而使训练过程更加稳定。

### 训练好的模型：链接：https://pan.baidu.com/s/1icAnMExdI1H6ZBAqfTEwDg 
提取码：1234 
